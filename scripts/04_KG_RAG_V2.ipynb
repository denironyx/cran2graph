{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "## Loading openai key\n",
    "with open('../environ.json') as f:\n",
    "    config = json.load(f)\n",
    "    api_keys = list(config.values())[1]\n",
    "\n",
    "# assign openai key to an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_keys\n",
    "\n",
    "# check to confirm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neo4j connections\n",
    "from neo4j import GraphDatabase\n",
    "host = 'bolt://127.0.0.1:7689'\n",
    "user = 'admin'\n",
    "password = 'cran2graph'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))\n",
    "\n",
    "def run_query(query, params={}):\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        result = session.run(query, params)\n",
    "        return result.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection to neo4j graph database\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://127.0.0.1:7689\", username=\"admin\", password=\"cran2graph\", database=\"neo4j\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Node properties are the following:\n",
      "        [{'labels': 'Person', 'properties': [{'property': 'name', 'type': 'STRING'}]}, {'labels': 'Package', 'properties': [{'property': 'embedding', 'type': 'LIST'}, {'property': 'latestPublishedDate', 'type': 'STRING'}, {'property': 'name', 'type': 'STRING'}, {'property': 'md5sum', 'type': 'STRING'}, {'property': 'description', 'type': 'STRING'}, {'property': 'license', 'type': 'STRING'}, {'property': 'version', 'type': 'STRING'}]}, {'labels': 'Organization', 'properties': [{'property': 'name', 'type': 'STRING'}]}, {'labels': 'License', 'properties': [{'property': 'type', 'type': 'STRING'}]}, {'labels': 'Institution', 'properties': [{'property': 'type', 'type': 'STRING'}]}]\n",
      "        Relationship properties are the following:\n",
      "        []\n",
      "        The relationships are the following:\n",
      "        ['(:Person)-[:CONTRIBUTED_TO]->(:Package)', '(:Person)-[:WORKS_AT]->(:Institution)', '(:Person)-[:MAINTAINS]->(:Package)', '(:Package)-[:DEPENDS_ON]->(:Package)', '(:Package)-[:LICENSE_BY]->(:License)', '(:Package)-[:SUPPORTED_BY]->(:Organization)']\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# check the graph schema\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "MATCH (p:Package)\n",
    "MATCH (p)-[r:CONTRIBUTED_TO|MAINTAINS]-(n:Person)\n",
    "WITH p, type(r) as type, collect(n.name) as names\n",
    "WITH p, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\n",
    "WITH p, collect(types) as contexts\n",
    "WITH p, \"Package Name: \"+ p.name + \" Last Published Date: \"+p.latestPublishedDate +\" Description: \"+ p.description +\"\\n\" +\n",
    "        reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") as context\n",
    "RETURN context Limit 5\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context\n",
      "0  Package Name: A3 Last Published Date: 2015-08-...\n",
      "1  Package Name: AalenJohansen Last Published Dat...\n",
      "2  Package Name: AATtools Last Published Date: 20...\n",
      "3  Package Name: ABACUS Last Published Date: 2019...\n",
      "4  Package Name: abasequence Last Published Date:...\n"
     ]
    }
   ],
   "source": [
    "print(run_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = api_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Server returned HTTP response code: 503 for URL: https://api.openai.com/v1/embeddings': 2,\n",
       " 'Server returned HTTP response code: 502 for URL: https://api.openai.com/v1/embeddings': 1,\n",
       " 'Server returned HTTP response code: 500 for URL: https://api.openai.com/v1/embeddings': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(openai_api_key)\n",
    "run_query(\"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "    'MATCH (p:Package) RETURN id(p) as id',\n",
    "    'MATCH (p:Package)\n",
    "    WHERE id(p) = id\n",
    "    MATCH (p)-[r:CONTRIBUTED_TO|MAINTAINS]-(n:Person)\n",
    "    WITH p, type(r) as type, collect(n.name) as names\n",
    "    WITH p, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\n",
    "    WITH p, collect(types) as contexts\n",
    "    WITH p, \"Package Name: \"+ p.name + \" Last Published Date: \"+p.latestPublishedDate +\" Description: \"+ p.description +\"\\n\" +\n",
    "            reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") as context\n",
    "    CALL apoc.ml.openai.embedding([context], $apiKey) YIELD embedding\n",
    "    SET p.embedding = embedding',\n",
    "    {batchSize:1, retries:1, params: {apiKey: $apiKey}})\n",
    "\"\"\", {'apiKey': openai_api_key})['errorMessages'][0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an assistant that helps to generate text to form nice and human understandable answers based.\n",
    "The latest prompt contains the information, and you need to generate a human readable response based on the given information.\n",
    "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "Do not add any additional information that is not explicitly provided in the latest prompt.\n",
    "I repeat, do not add any information that is not explicitly given.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "    The question is {question}\n",
    "    Answer the question by using the provided information:\n",
    "    {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k=3):\n",
    "    data = run_query(\"\"\"\n",
    "        // retrieve the embedding of the question\n",
    "        CALL apoc.ml.openai.embedding([$question], $apiKey) YIELD embedding \n",
    "        // match relevant packages\n",
    "        MATCH (p:Package)\n",
    "        WITH p, gds.similarity.cosine(embedding, p.embedding) AS score\n",
    "        ORDER BY score DESC\n",
    "        // LIMIT the number of relevant description\n",
    "        LIMIT toInteger($k)\n",
    "        // retrieve graph context \n",
    "        MATCH (p)--()--(p1:Package)\n",
    "        WITH p, p1, count(*) AS count\n",
    "        ORDER BY count DESC\n",
    "        WITH p, apoc.text.join(collect(p1.package)[..3], \", \") as similarPackage\n",
    "        MATCH (p)-[r:CONTRIBUTED_TO|MAINTAINS]-(n:Person)\n",
    "        WITH p, similarPackage, type(r) as type, collect(n.name) as names\n",
    "        WITH p, similarPackage, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\n",
    "        WITH p, similarPackage, collect(types) as contexts\n",
    "        WITH p, similarPackage, \"Package Name: \"+ p.name + \" Last Published Date: \"+p.latestPublishedDate +\" Description: \"+ p.description +\"\\n\" +\n",
    "                reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\")  + \"similar packages:\" + similarPackage + \"\\n\" as context\n",
    "        RETURN context  \n",
    "    \"\"\", {'question': question, 'k': k, 'apiKey': api_keys})\n",
    "    return data['context'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    # Retrieve context\n",
    "    context = retrieve_context(question)\n",
    "    # Print context\n",
    "    for c in context:\n",
    "        print(c)\n",
    "    # Generate answer\n",
    "    response = run_query(\n",
    "        \"\"\"\n",
    "  CALL apoc.ml.openai.chat([{role:'system', content: $system},\n",
    "                      {role: 'user', content: $user}], $apiKey) YIELD value\n",
    "  RETURN value.choices[0].message.content AS answer\n",
    "  \"\"\",\n",
    "        {\n",
    "            \"system\": system_prompt,\n",
    "            \"user\": generate_user_prompt(question, context),\n",
    "            \"apiKey\": api_keys,\n",
    "        },\n",
    "    )\n",
    "    return response[\"answer\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Name: dplyr Last Published Date: 2023-09-03 Description: A fast, consistent tool for working with data frame like\n",
      "    objects, both in memory and out of memory.\n",
      "CONTRIBUTED_TO: Davis Vaughan, Hadley Wickham, Romain François, Lionel Henry, Kirill Müller\n",
      "MAINTAINS: Hadley Wickham\n",
      "similar packages:\n",
      "\n",
      "Package Name: crplyr Last Published Date: 2023-03-21 Description: In order to facilitate analysis of datasets hosted on the Crunch\n",
      "    data platform <https://crunch.io/>, the crplyr package implements dplyr\n",
      "    methods on top of the Crunch backend. The usual methods select, filter,\n",
      "    group_by, summarize, and collect are implemented in such a way as to\n",
      "    perform as much computation on the server and pull as little data locally\n",
      "    as possible.\n",
      "MAINTAINS: Greg Freedman Ellis\n",
      "CONTRIBUTED_TO: Neal Richardson, Mike Malecki, Greg Freedman Ellis, Aljaz Sluga, Jonathan Keane, Gordon Shotwell\n",
      "similar packages:\n",
      "\n",
      "Package Name: dtrackr Last Published Date: 2023-09-04 Description: Track and\n",
      "    document dplyr data pipelines. As you filter, mutate, and join your\n",
      "    way through a data set, dtrackr seamlessly keeps track of your data\n",
      "    flow and makes publication ready documentation of a data pipeline simple.\n",
      "MAINTAINS: Robert Challen\n",
      "CONTRIBUTED_TO: Robert Challen\n",
      "similar packages:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dplyr package is maintained by Hadley Wickham.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"Who maintains dplyr package?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Name: sf Last Published Date: 2023-07-11 Description: Support for simple features, a standardized way to\n",
      "    encode spatial vector data. Binds to GDAL for reading and writing\n",
      "    data, to GEOS for geometrical operations, and to PROJ for\n",
      "    projection conversions and datum transformations. Uses by default the s2\n",
      "    package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n",
      "CONTRIBUTED_TO: Hadley Wickham, Ian Cook, Jeroen Ooms, Thomas Lin Pedersen, Tim Keitt, Etienne Racine, Robin Lovelace, Dewey Dunnington, Michael Sumner, Roger Bivand, Dan Baston, Edzer Pebesma, Kirill Müller\n",
      "MAINTAINS: Edzer Pebesma\n",
      "similar packages:\n",
      "\n",
      "Package Name: mapsf Last Published Date: 2023-09-05 Description: Create and integrate thematic maps in your workflow. This package\n",
      "    helps to design various cartographic representations such as proportional\n",
      "    symbols, choropleth or typology maps. It also offers several functions to\n",
      "    display layout elements that improve the graphic presentation of maps\n",
      "    (e.g. scale bar, north arrow, title, labels). mapsf maps sf objects on\n",
      "    base graphics.\n",
      "MAINTAINS: Timothée Giraud\n",
      "CONTRIBUTED_TO: Hugues Pecout, from wordcloud\n",
      "    package), from plotrix\n",
      "    package), Florian Zenoni, Ronan Ysebaert, Jim Lemon  (Arc drawing algorithm for annotations, Diego Hernangómez, Ian Fellows  (No overlap algorithm for labels, Timothée Giraud\n",
      "similar packages:\n",
      "\n",
      "Package Name: geospark Last Published Date: 2020-03-02 Description: R binds GeoSpark <http://geospark.datasyslab.org/> extending sparklyr \n",
      "    <https://spark.rstudio.com/> R package to make distributed geocomputing easier. Sf is a\n",
      "    package that provides [simple features] <https://en.wikipedia.org/wiki/Simple_Features> access\n",
      "    for R and which is a leading geospatial data processing tool. Geospark R package bring \n",
      "    the same simple features access like sf but running on Spark distributed system.\n",
      "MAINTAINS: Harry Zhu\n",
      "CONTRIBUTED_TO: Harry Zhu, Javier Luraschi\n",
      "similar packages:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Some packages that are similar to the `sf` package are:\\n\\n1. `mapsf` - This package helps to design various cartographic representations and integrates thematic maps into your workflow. It offers functions to display layout elements that improve the graphic presentation of maps. It maps `sf` objects on base graphics. This package is maintained by Timothée Giraud.\\n\\n2. `geospark` - This package extends the `sparklyr` package to make distributed geocomputing easier. It provides the same access to simple features like `sf`, but it runs on the Spark distributed system. This package is maintained by Harry Zhu.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"what packages are similar to sf package?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Name: sna Last Published Date: 2023-01-24 Description: A range of tools for social network analysis, including node and graph-level indices, structural distance and covariance methods, structural equivalence detection, network regression, random graph generation, and 2D/3D network visualization.\n",
      "MAINTAINS: Carter T. Butts\n",
      "CONTRIBUTED_TO: Carter T. Butts\n",
      "similar packages:\n",
      "\n",
      "Package Name: networktools Last Published Date: 2023-08-22 Description: Includes assorted tools for network analysis. Bridge centrality; goldbricker; MDS, PCA, & eigenmodel network plotting.\n",
      "MAINTAINS: Payton Jones\n",
      "CONTRIBUTED_TO: Payton Jones\n",
      "similar packages:\n",
      "\n",
      "Package Name: NetworkToolbox Last Published Date: 2021-05-28 Description: Implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. Methods include various filtering methods and approaches such as threshold, dependency (Kenett, Tumminello, Madi, Gur-Gershgoren, Mantegna, & Ben-Jacob, 2010 <doi:10.1371/journal.pone.0015032>), Information Filtering Networks (Barfuss, Massara, Di Matteo, & Aste, 2016 <doi:10.1103/PhysRevE.94.062306>), and Efficiency-Cost Optimization (Fallani, Latora, & Chavez, 2017 <doi:10.1371/journal.pcbi.1005305>). Brain methods include the recently developed Connectome Predictive Modeling (see references in package). Also implements several network measures including local network characteristics (e.g., centrality), community-level network characteristics (e.g., community centrality), global network characteristics (e.g., clustering coefficient), and various other measures associated with the reliability and reproducibility of network analysis. \n",
      "MAINTAINS: Alexander Christensen\n",
      "CONTRIBUTED_TO: Guido Previde Massara, Alexander Christensen\n",
      "similar packages:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, I would recommend the following packages for network science analysis:\\n\\n1. Package Name: sna\\n   Last Published Date: 2023-01-24\\n   Description: This package provides a range of tools for social network analysis. It includes node and graph-level indices, structural distance and covariance methods, structural equivalence detection, network regression, random graph generation, and 2D/3D network visualization.\\n\\n2. Package Name: networktools\\n   Last Published Date: 2023-08-22\\n   Description: This package includes assorted tools for network analysis. It offers features such as bridge centrality, goldbricker, and options for MDS, PCA, and eigenmodel network plotting.\\n\\n3. Package Name: NetworkToolbox\\n   Last Published Date: 2021-05-28\\n   Description: This package implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. It provides various filtering methods and approaches, such as threshold and dependency. It also offers efficiency-cost optimization and Connectome Predictive Modeling for brain analysis. Additionally, it includes several network measures for local, community-level, and global network characteristics.\\n\\nThese packages cover a wide range of network analysis tools and methods, so you can choose based on your specific needs and research interests.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"recommend packages for network science analysis?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
