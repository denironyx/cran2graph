{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neo4j import GraphDatabase\n",
    "\n",
    "# class Neo4jConnect:\n",
    "#     def __init__(self, user, password):\n",
    "#         self.driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(user, password))\n",
    "#         self.user = user\n",
    "#     def close(self):\n",
    "#         self.driver.close()\n",
    "        \n",
    "#     def query(self, query):\n",
    "#         session = self.driver.session(database=\"cran\")\n",
    "#         result = session.run(query)\n",
    "#         return result\n",
    "    \n",
    "#     def clean_database(self):\n",
    "#         \"\"\"\n",
    "#         Delete all nodes and edge in Neo4j test container\n",
    "#         \"\"\"\n",
    "#         q = \"MATCH (n) DETACH DELETE (n)\"\n",
    "#         try:\n",
    "#             session = self.driver.session()\n",
    "#             session.run(q)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "    \n",
    "#     def test_neo4j_connection(self):\n",
    "#         \"\"\"Test neo4j connection\n",
    "#         \"\"\"\n",
    "#         session = self.driver.session()\n",
    "#         result = session.run(\"Match () Return 1 Limit 1\")\n",
    "#         return result\n",
    "#     def __str__(self):\n",
    "#         return f'Connection successful for user: {self.user}.\\nConnected to Neo4J database uri: {self.uri}'\n",
    "    \n",
    "# connection = Neo4jConnect('admin', 'cran2graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up connections\n",
    "\n",
    "from graphutils.utils import Neo4jConnect\n",
    "\n",
    "connection = Neo4jConnect('admin', 'cran2graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': {'person': 'Jing Zhang'},\n",
       "  'p': {'license': 'GPL-3',\n",
       "   'package': 'do',\n",
       "   'md5sum': 'f89af897b6b03444fdfa63d91d51ef0a',\n",
       "   'description': 'Flexibly convert data between long and wide format using just two\\n    functions: reshape_toLong() and reshape_toWide().',\n",
       "   'published': '2021-08-03',\n",
       "   'version': '2.0.0.0'}},\n",
       " {'a': {'person': 'Jing Zhang'},\n",
       "  'p': {'license': 'GPL-3',\n",
       "   'package': 'nomogramFormula',\n",
       "   'md5sum': '552ddb326bbff202be3fe31ec2f36ff3',\n",
       "   'description': '\\n  A nomogram, which can be carried out in rms package, provides a \\n    graphical explanation of a prediction process. However, it is not very easy\\n    to draw straight lines, read points and probabilities accurately. Even, it \\n    is hard for users to calculate total points and probabilities for all \\n    subjects.\\n  This package provides formula_rd() and formula_lp() functions to fit the \\n    formula of total points with raw data and linear predictors respectively by\\n    polynomial regression. Function points_cal() will help you calculate the \\n    total points. prob_cal() can be used to calculate the probabilities after\\n    lrm(), cph() or psm() regression. \\n  For more complex condition, interaction or restricted cubic spine, TotalPoints.rms() \\n    can be used.',\n",
       "   'published': '2020-01-28',\n",
       "   'version': '1.2.0.0'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the connection to neo4j\n",
    "test_query = \"\"\"\n",
    "MATCH (a:Person {person: 'Jing Zhang'})-[:MAINTAINS]->(p:Package)\n",
    "RETURN a,p\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "package_maintainer = connection.query(test_query).data()\n",
    "package_maintainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create constraint on the various nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the constraints with: CREATE CONSTRAINT IF NOT EXISTS FOR (d:Dependency) REQUIRE d.name IS UNIQUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the constraints with: CREATE CONSTRAINT IF NOT EXISTS FOR (m:Import) REQUIRE m.package IS UNIQUE\n",
      "Creating the constraints with: CREATE CONSTRAINT IF NOT EXISTS FOR (l:License) REQUIRE l.license IS UNIQUE\n",
      "Creating the constraints with: CREATE CONSTRAINT IF NOT EXISTS FOR (i:Institution) REQUIRE i.institution IS UNIQUE\n",
      "Creating the constraints with: CREATE CONSTRAINT IF NOT EXISTS FOR (a:Person) REQUIRE a.person IS UNIQUE\n",
      "Creating the constraints with: CREATE CONSTRAINT IF NOT EXISTS FOR (p:Package) REQUIRE p.package IS UNIQUE\n"
     ]
    }
   ],
   "source": [
    "# Create a constraint on 'Package' nodes based on the 'package' property\n",
    "constraint_package_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (p:Package) REQUIRE p.package IS UNIQUE\"\n",
    "\n",
    "# Create a constraint on 'Person' nodes based on the 'person' property\n",
    "constraint_person_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (a:Person) REQUIRE a.person IS UNIQUE\"\n",
    "\n",
    "# Create a constraint on 'Person' nodes based on the 'person' property\n",
    "constraint_license_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (l:License) REQUIRE l.license IS UNIQUE\"\n",
    "\n",
    "# # Create a constraint on 'Person' nodes based on the 'person' property\n",
    "# constraint_license_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (l:License) REQUIRE l.license IS UNIQUE\"\n",
    "\n",
    "# Create a constraint on 'Person' nodes based on the 'person' property\n",
    "constraint_institution_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (i:Institution) REQUIRE i.institution IS UNIQUE\"\n",
    "\n",
    "# Create a constraint on 'Person' nodes based on the 'person' property\n",
    "constraint_imports_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Import) REQUIRE m.package IS UNIQUE\"\n",
    "\n",
    "# Create a constraint on 'Person' nodes based on the 'person' property\n",
    "constraint_dependency_query = \"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Dependency) REQUIRE d.name IS UNIQUE\"\n",
    "\n",
    "# Create an index on the 'person' property of 'Person' nodes\n",
    "person_index_query = \"CREATE INDEX ON :Person(person)\"\n",
    "\n",
    "constraints = [constraint_dependency_query, constraint_imports_query, constraint_license_query, constraint_institution_query, constraint_person_query, constraint_package_query]\n",
    "\n",
    "# for constraint in constraints:\n",
    "#     print(f\"Creating the constraints with: {constraint}\")\n",
    "#     connection.query(constraint)\n",
    "# connection.close()\n",
    "\n",
    "\n",
    "#db_session = establish_connection()\n",
    "\n",
    "for constraint in constraints:\n",
    "    print(f\"Creating the constraints with: {constraint}\")\n",
    "    connection.query(constraint)\n",
    "#db_session.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading package nodes from csv file\n",
    "load_package_query = \"\"\"\n",
    "// Load 'Package' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (p:Package {\n",
    "        package: row.package,\n",
    "        version: row.version,\n",
    "        license: row.license,\n",
    "        md5sum: row.md5sum,\n",
    "        description: row.description,\n",
    "        published: row.published_date\n",
    "    })\n",
    "    ON CREATE SET\n",
    "        p.package = row.package,\n",
    "        p.version = row.version,\n",
    "        p.license = row.license,\n",
    "        p.md5sum = row.md5sum,\n",
    "        p.description = row.description,\n",
    "        p.published = row.published_date\n",
    "    ON MATCH SET\n",
    "        p.package = row.package,\n",
    "        p.version = row.version,\n",
    "        p.license = row.license,\n",
    "        p.md5sum = row.md5sum,\n",
    "        p.description = row.description,\n",
    "        p.published = row.published_date\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_package_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n",
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## THis fix the issue\n",
    "## Loading package nodes from csv file\n",
    "load_package_query = \"\"\"\n",
    "// Load 'Package' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (p:Package {package: row.package})\n",
    "    ON CREATE SET\n",
    "        p.version = row.version,\n",
    "        p.license = row.license,\n",
    "        p.md5sum = row.md5sum,\n",
    "        p.description = row.description,\n",
    "        p.published = row.published_date\n",
    "    ON MATCH SET\n",
    "        p.version = row.version,\n",
    "        p.license = row.license,\n",
    "        p.md5sum = row.md5sum,\n",
    "        p.description = row.description,\n",
    "        p.published = row.published_date\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_package_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n",
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## Loading package nodes from csv file\n",
    "load_package_query = \"\"\"\n",
    "// Load 'Package' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (p:Package {\n",
    "        package: row.package,\n",
    "        version: row.version,\n",
    "        license: row.license,\n",
    "        md5sum: row.md5sum,\n",
    "        description: row.description,\n",
    "        published: row.published_date\n",
    "    })\n",
    "    ON CREATE SET\n",
    "        package = row.package,\n",
    "        version = row.version,\n",
    "        license = row.license,\n",
    "        md5sum = row.md5sum,\n",
    "        description = row.description\n",
    "        published = row.published_date\n",
    "    ON MATCH SET\n",
    "        package = row.package,\n",
    "        version = row.version,\n",
    "        license = row.license,\n",
    "        md5sum = row.md5sum,\n",
    "        description = row.description\n",
    "        published = row.published_date\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_package_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n",
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## working comment\n",
    "## Loading person nodes from csv file\n",
    "load_person_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (a:Person {\n",
    "        person: COALESCE(row.author, \"N/A\")\n",
    "    })\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_person_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## working comment\n",
    "## Loading imports nodes from csv file\n",
    "load_dependency_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (a:Dependency {\n",
    "        name: COALESCE(row.imports, \"N/A\")\n",
    "    })\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_dependency_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "ename": "SessionError",
     "evalue": "Session closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSessionError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dee\\root\\Projects\\dev\\cran2graph\\scripts\\02_Loading_Data_Neo4j copy.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m load_person_query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m// Load \u001b[39m\u001b[39m'\u001b[39m\u001b[39mperson\u001b[39m\u001b[39m'\u001b[39m\u001b[39m nodes from the CSV file\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mLOAD CSV WITH HEADERS FROM \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfile:///cran_process_data.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m AS row\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m} IN TRANSACTIONS OF 10000 ROWS;\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading query....\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m db_session\u001b[39m.\u001b[39;49mrun(load_person_query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m db_session\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dee/root/Projects/dev/cran2graph/scripts/02_Loading_Data_Neo4j%20copy.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend operation....\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dee\\root\\Projects\\dev\\cran2graph\\venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:289\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[0;32m    255\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    256\u001b[0m     query: t\u001b[39m.\u001b[39mUnion[te\u001b[39m.\u001b[39mLiteralString, Query],\n\u001b[0;32m    257\u001b[0m     parameters: t\u001b[39m.\u001b[39mOptional[t\u001b[39m.\u001b[39mDict[\u001b[39mstr\u001b[39m, t\u001b[39m.\u001b[39mAny]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: t\u001b[39m.\u001b[39mAny\n\u001b[0;32m    259\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Result:\n\u001b[0;32m    260\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run a Cypher query within an auto-commit transaction.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[39m    The query is sent and the result header received\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m    :raises SessionError: if the session has been closed.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_state()\n\u001b[0;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m query:\n\u001b[0;32m    291\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot run an empty query\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dee\\root\\Projects\\dev\\cran2graph\\venv\\Lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:209\u001b[0m, in \u001b[0;36mWorkspace._check_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_state\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed:\n\u001b[1;32m--> 209\u001b[0m         \u001b[39mraise\u001b[39;00m SessionError(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSession closed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mSessionError\u001b[0m: Session closed"
     ]
    }
   ],
   "source": [
    "## THis fix the issue\n",
    "## Loading package nodes from csv file\n",
    "## Loading person nodes from csv file\n",
    "load_person_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (a:Person {\n",
    "        person: COALESCE(row.author, \"N/A\")\n",
    "    })\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_person_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## THis fix the issue\n",
    "## Loading package nodes from csv file\n",
    "## Loading person nodes from csv file\n",
    "load_license_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (l:License {\n",
    "        license: COALESCE(row.license, \"N/A\")\n",
    "    })\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_license_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## THis fix the issue\n",
    "## Loading package nodes from csv file\n",
    "## Loading person nodes from csv file\n",
    "load_institution_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (i:Institution {\n",
    "        institution: COALESCE(row.institution, \"N/A\")\n",
    "    })\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_institution_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THis fix the issue\n",
    "## Loading package nodes from csv file\n",
    "## Loading person nodes from csv file\n",
    "load_institution_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "CALL {\n",
    "    WITH row\n",
    "    MERGE (i:Institution {\n",
    "        institution: COALESCE(row.institution, \"N/A\")\n",
    "    })\n",
    "} IN TRANSACTIONS OF 10000 ROWS;\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(load_institution_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## Loading contributed_to relationships\n",
    "rel_contributed_to_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "MATCH (p:Package { package: row.package })\n",
    "MATCH (a:Person { person: COALESCE(row.author, \"N/A\")})\n",
    "\n",
    "MERGE (a)-[:CONTRIBUTED_TO]->(p);\n",
    "\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(rel_contributed_to_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## Loading contributed_to relationships\n",
    "rel_maintains_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "MATCH (p:Package { package: row.package })\n",
    "MATCH (a:Person { person: COALESCE(row.maintainer_name,\"N/A\")})\n",
    "\n",
    "MERGE (a)-[:MAINTAINS]->(p);\n",
    "\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(rel_maintains_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## Loading contributed_to relationships\n",
    "rel_license_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "MATCH (p:Package { package: row.package })\n",
    "MATCH (l:License { license: COALESCE(row.license,\"N/A\")})\n",
    "\n",
    "MERGE (p)-[:LICENSE_BY]->(l);\n",
    "\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(rel_license_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## Loading contributed_to relationships\n",
    "rel_institution_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "MATCH (a:Person { person: COALESCE(row.maintainer_name,\"N/A\")})\n",
    "MATCH (i:Institution { institution: COALESCE(row.institution ,\"N/A\")})\n",
    "\n",
    "MERGE (a)-[:WORKS_IN]->(i);\n",
    "\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(rel_institution_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading query....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\808217691.py:11: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  session = self.driver.session(database=\"cran\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end operation....\n"
     ]
    }
   ],
   "source": [
    "## Loading contributed_to relationships\n",
    "rel_dependency_query = \"\"\"\n",
    "// Load 'person' nodes from the CSV file\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cran_process_data.csv' AS row\n",
    "MATCH (p:Package { package: row.package })\n",
    "MATCH (d:Dependency { name: COALESCE(row.imports,\"N/A\")})\n",
    "\n",
    "MERGE (p)-[:DEPENDS_ON]->(d);\n",
    "\n",
    "\"\"\"\n",
    "print(f\"loading query....\")\n",
    "connection.query(rel_dependency_query)\n",
    "connection.close()\n",
    "print(f\"end operation....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dee\\AppData\\Local\\Temp\\ipykernel_92308\\1262747138.py:21: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  data = session.read_transaction(get_package_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'p.package': 'A3', 'p.description': 'Supplies tools for tabulating and analyzing the results of predictive models. The methods employed are applicable to virtually any predictive model and make comparisons between different methodologies straightforward.'}, {'p.package': 'AalenJohansen', 'p.description': 'Provides the conditional Nelson-Aalen and Aalen-Johansen estimators. The methods are based on Bladt & Furrer (2023), in preparation.'}, {'p.package': 'AATtools', 'p.description': 'Compute approach bias scores using different scoring algorithms,\\n    compute bootstrapped and exact split-half reliability estimates,\\n    and compute confidence intervals for individual participant scores.'}, {'p.package': 'ABACUS', 'p.description': 'A set of Shiny apps for effective communication and understanding in statistics. The current version includes properties of normal distribution, properties of sampling distribution, one-sample z and t tests, two samples independent (unpaired) t test and analysis of variance.'}, {'p.package': 'abasequence', 'p.description': 'Provides a suite of functions for analyzing sequences of events. Users can generate and code sequences based on predefined rules, with a special focus on the identification of sequences coded as ABA (when one element appears, followed by a different one, and then followed by the first). Additionally, the package offers the ability to calculate the length of consecutive ABA-coded sequences sharing common elements. The methods implemented in this package are based on the work by Ziembowicz, K., Rychwalska, A., & Nowak, A. (2022). <doi:10.1177/10464964221118674>.'}, {'p.package': 'abbreviate', 'p.description': 'Strings are abbreviated to at least minlength characters, such that they remain unique \\n    (if they were). The abbreviations should be recognisable.'}, {'p.package': 'abbyyR', 'p.description': 'Get text from images of text using Abbyy Cloud Optical Character\\n    Recognition (OCR) API. Easily OCR images, barcodes, forms, documents with\\n    machine readable zones, e.g. passports. Get the results in a variety of formats\\n    including plain text and XML. To learn more about the Abbyy OCR API, see \\n    <http://ocrsdk.com/>.'}, {'p.package': 'abc', 'p.description': 'Implements several ABC algorithms for\\n        performing parameter estimation, model selection, and goodness-of-fit.\\n        Cross-validation tools are also available for measuring the\\n        accuracy of ABC estimates, and to calculate the\\n        misclassification probabilities of different models.'}, {'p.package': 'abc.data', 'p.description': 'Contains data which are used by functions of the abc package.'}, {'p.package': 'ABC.RAP', 'p.description': 'It aims to identify candidate genes that are “differentially\\n    methylated” between cases and controls. It applies Student’s t-test and delta beta analysis to\\n    identify candidate genes containing multiple “CpG sites”.'}, {'p.package': 'ABCanalysis', 'p.description': 'For a given data set, the package provides a novel method of computing precise limits to acquire subsets which are easily interpreted. Closely related to the Lorenz curve, the ABC curve visualizes the data by graphically representing the cumulative distribution function. Based on an ABC analysis the algorithm calculates, with the help of the ABC curve, the optimal limits by exploiting the mathematical properties pertaining to distribution of analyzed items. The data containing positive values is divided into three disjoint subsets A, B and C, with subset A comprising very profitable values, i.e. largest data values (the important few), subset B comprising values where the yield equals to the effort required to obtain it, and the subset C comprising of non-profitable values, i.e., the smallest data sets (the trivial many). Package is based on Computed ABC Analysis for rational Selection of most informative Variables in multivariate Data, PLoS One. Ultsch. A., Lotsch J. (2015) <DOI:10.1371/journal.pone.0129767>.'}, {'p.package': 'abclass', 'p.description': 'Multi-category angle-based large-margin classifiers.\\n    See Zhang and Liu (2014) <doi:10.1093/biomet/asu017> for details.'}, {'p.package': 'ABCoptim', 'p.description': 'An implementation of Karaboga (2005) Artificial Bee Colony\\n    Optimization algorithm <http://mf.erciyes.edu.tr/abc/pub/tr06_2005.pdf>.\\n    This (working) version is a Work-in-progress, which is\\n    why it has been implemented using pure R code. This was developed upon the basic\\n    version programmed in C and distributed at the algorithms official website.'}, {'p.package': 'ABCp2', 'p.description': 'Tests the goodness of fit of a distribution of offspring to the Normal, Poisson, and Gamma distribution and estimates the proportional paternity of the second male (P2) based on the best fit distribution.'}, {'p.package': 'abcrf', 'p.description': 'Performs Approximate Bayesian Computation (ABC) model choice and parameter inference via random forests.\\n  Pudlo P., Marin J.-M., Estoup A., Cornuet J.-M., Gautier M. and Robert C. P. (2016) <doi:10.1093/bioinformatics/btv684>.\\n  Estoup A., Raynal L., Verdu P. and Marin J.-M. <http://journal-sfds.fr/article/view/709>.\\n  Raynal L., Marin J.-M., Pudlo P., Ribatet M., Robert C. P. and Estoup A. (2019) <doi:10.1093/bioinformatics/bty867>.'}, {'p.package': 'abcrlda', 'p.description': 'Offers methods to perform asymptotically bias-corrected regularized linear discriminant analysis (ABC_RLDA) for cost-sensitive binary classification. The bias-correction is an estimate of the bias term added to regularized discriminant analysis (RLDA) that minimizes the overall risk. The default magnitude of misclassification costs are equal and set to 0.5; however, the package also offers the options to set them to some predetermined values or, alternatively, take them as hyperparameters to tune.\\n    A. Zollanvari, M. Abdirash, A. Dadlani and B. Abibullaev (2019) <doi:10.1109/LSP.2019.2918485>.'}, {'p.package': 'abctools', 'p.description': 'Tools for approximate Bayesian computation including summary statistic selection and assessing coverage.\\n    See Nunes and Prangle (2015) <doi:10.32614/RJ-2015-030> and Rodrigues, Prangle and Sisson (2018) <doi:10.1016/j.csda.2018.04.004>.'}, {'p.package': 'abd', 'p.description': 'The abd package contains data sets and sample code for The\\n    Analysis of Biological Data by Michael Whitlock and Dolph Schluter (2009;\\n    Roberts & Company Publishers).'}, {'p.package': 'abdiv', 'p.description': 'A collection of measures for measuring ecological diversity.\\n  Ecological diversity comes in two flavors: alpha diversity measures the\\n  diversity within a single site or sample, and beta diversity measures the\\n  diversity across two sites or samples. This package overlaps considerably\\n  with other R packages such as vegan, gUniFrac, betapart, and fossil.\\n  We also include a wide range of functions that are implemented in software\\n  outside the R ecosystem, such as scipy, Mothur, and scikit-bio.  The\\n  implementations here are designed to be basic and clear to the reader.'}, {'p.package': 'abe', 'p.description': 'Performs augmented backward elimination and checks the stability of the obtained model. Augmented backward elimination combines significance or information based criteria with the change in estimate to either select the optimal model for prediction purposes or to serve as a tool to obtain a practically sound, highly interpretable model. More details can be found in Dunkler et al. (2014) <doi:10.1371/journal.pone.0113677>.'}, {'p.package': 'abess', 'p.description': 'Extremely efficient toolkit for solving the best subset selection problem <https://www.jmlr.org/papers/v23/21-1060.html>. This package is its R interface. The package implements and generalizes algorithms designed in <doi:10.1073/pnas.2014241117> that exploits a novel sequencing-and-splicing technique to guarantee exact support recovery and globally optimal solution in polynomial times for linear model. It also supports best subset selection for logistic regression, Poisson regression, Cox proportional hazard model, Gamma regression, multiple-response regression, multinomial logistic regression, ordinal regression, (sequential) principal component analysis, and robust principal component analysis. The other valuable features such as the best subset of group selection <doi:10.1287/ijoc.2022.1241> and sure independence screening <doi:10.1111/j.1467-9868.2008.00674.x> are also provided.'}, {'p.package': 'abglasso', 'p.description': 'Implements a Bayesian adaptive graphical lasso data-augmented block Gibbs sampler. The sampler simulates the posterior distribution of precision matrices of a Gaussian Graphical Model. This sampler was adapted from the original MATLAB routine proposed in Wang (2012) <doi:10.1214/12-BA729>.'}, {'p.package': 'ABHgenotypeR', 'p.description': 'Easy to use functions to visualize marker data\\n    from biparental populations. Useful for both analyzing and\\n    presenting genotypes in the ABH format.'}, {'p.package': 'abind', 'p.description': 'Combine multidimensional arrays into a single array.\\n  This is a generalization of cbind and rbind.  Works with\\n  vectors, matrices, and higher-dimensional arrays.  Also\\n  provides functions adrop, asub, and afill for manipulating,\\n  extracting and replacing data in arrays.'}, {'p.package': 'abjData', 'p.description': 'The Brazilian Jurimetrics Association (ABJ in\\n    Portuguese, see <https://abj.org.br/> for more information) is\\n    a non-profit organization which aims to investigate and promote the\\n    use of statistics and probability in the study of Law and its\\n    institutions. This package has a set of datasets commonly used in\\n    our book.'}]\n"
     ]
    }
   ],
   "source": [
    "import igraph\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Define a function to retrieve data from Neo4j\n",
    "def get_package_data(tx):\n",
    "    query = \"MATCH (p:Package) RETURN p.package, p.description limit 25\"\n",
    "    result = tx.run(query)\n",
    "    return result.data()\n",
    "\n",
    "# Connect to your Neo4j database\n",
    "uri = \"neo4j://localhost:7687\"\n",
    "username = \"admin\"\n",
    "password = \"cran2graph\"\n",
    "\n",
    "# Create a Neo4j driver\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "\n",
    "# Retrieve package data from Neo4j\n",
    "with driver.session(database=\"cran\") as session:\n",
    "    data = session.read_transaction(get_package_data)\n",
    "\n",
    "# Preprocess the input text (e.g., remove stopwords and tokenize)\n",
    "input_text = \"package to perform data transformation and cleaning\"\n",
    "\n",
    "# Create a list of package descriptions\n",
    "package_descriptions = [package[\"description\"] for package in data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
